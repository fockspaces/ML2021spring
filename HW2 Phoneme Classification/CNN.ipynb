{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fockspaces/ML2021spring/blob/main/HW2%20Phoneme%20Classification/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n",
        "* Video (English): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqKyrT9mDtM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk_CWeC8mDR0",
        "outputId": "793a32f8-e16f-48b2-f340-1cb0edeec198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "5990291b-b949-4e3d-b8fe-71b061f0a216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  drive  sample_data  timit_11\n"
          ]
        }
      ],
      "source": [
        "# !gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!cp \"/content/drive/MyDrive/timit_11_v2.zip 的副本\" \"data.zip\"\n",
        "!unzip data.zip\n",
        "!ls "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "0ddf1420-ff1a-49e0-b6b2-0e09cd6862fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3WB9xXUlIqAT"
      },
      "outputs": [],
      "source": [
        "def sample(data, stride):\n",
        "  base = data.copy()\n",
        "  for step in range(1, stride+1, 1):\n",
        "    Rshift = np.roll(base,step,axis=0)\n",
        "    data = np.concatenate((Rshift,data), axis=1)\n",
        "  for step in range(-1, -1-1*stride, -1):\n",
        "    Lshift = np.roll(base,step,axis=0)\n",
        "    data = np.concatenate((data,Lshift), axis=1)\n",
        "\n",
        "  data = np.reshape(data, (-1,2*stride+1,39))\n",
        "  return data\n",
        "\n",
        "def normalize(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  std = np.std(data, axis=0)\n",
        "  return (data - mean)/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y_0i5dpHQPBu"
      },
      "outputs": [],
      "source": [
        "stride = 25\n",
        "# (,429)split to(11,39)\n",
        "train = np.reshape(train, (-1,11,39))\n",
        "test = np.reshape(test, (-1,11,39))\n",
        "\n",
        "# pick only the 5th MFCC which is corresponding to the label\n",
        "train = train[:,5,:]\n",
        "test = test[:,5,:]\n",
        "\n",
        "# include nearby MFCC (To extend the frame length)\n",
        "train = sample(train, stride)\n",
        "test = sample(test, stride)\n",
        "\n",
        "# transform to 4d input array (with kernel)\n",
        "train = np.reshape(train, (-1,1,(2*stride+1),39))\n",
        "test = np.reshape(test, (-1,1,(2*stride+1),39))\n",
        "\n",
        "# normalize data\n",
        "# train = normalize(train)\n",
        "# test = normalize(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsKh0UbeO5VY",
        "outputId": "65170e52-bf79-4961-d829-af601b26a5a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1229932, 1, 51, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "siKG6cmMepVv",
        "outputId": "9ecf912f-314d-4690-c9d3-ada105252c81"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "mfcc = np.reshape(train[0], ((2*stride+1),39))\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.imshow(mfcc);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sYqi_lAuvC59"
      },
      "outputs": [],
      "source": [
        "VAL_RATIO = 0\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "# train_x, train_y, val_x, val_y = train[:], train_label[:], train[percent:], train_label[percent:]\n",
        "val_x, val_y = train[percent:], train_label[percent:]\n",
        "# print('Size of training set: {}'.format(train_x.shape))\n",
        "# print('Size of validation set: {}'.format(val_x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E04qGvVN212c"
      },
      "outputs": [],
      "source": [
        "# train_x = train_x.reshape(train_x.shape[0], 1, train_x.shape[1], -1)\n",
        "# test_x = test.reshape(test.shape[0], 1, test.shape[1], -1)\n",
        "# train_x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUCbQvqJurYc",
        "outputId": "1e36dd49-bba0-4aca-d27c-0a9c663f61ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train, train_label)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "39a458b0-a15c-4a94-d54d-1461c2c8a212"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, val_x, val_y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.layer3 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "\n",
        "        self.act_fn = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SfJfs6CcXZZj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # The arguments for commonly used modules:\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "        # input image size: [1, num, 29]\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            #1st\n",
        "            nn.Conv2d(1, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            #2nd\n",
        "            nn.Conv2d(32, 32, 5, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            #3rd\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #4th\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            #5rd\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #6th\n",
        "            nn.Conv2d(128, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(48 * 8*8, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 39)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input (x): [batch_size, 3, 128, 128]\n",
        "        # output: [batch_size, 11]\n",
        "\n",
        "        # Extract features by convolutional layers.\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
        "        x = self.fc_layers(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "outputs": [],
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "outputs": [],
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp3ZXg1yO9Y",
        "outputId": "6f71d06c-f977-4d14-d7c6-b344c47ebcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 300               # number of training epoch\n",
        "learning_rate = 1e-4       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './CNNmodel_25.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "b5a51354-13b9-46c6-ca50-08293cdb1924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/300] Train Acc: 0.434875 Loss: 2.045172\n",
            "[002/300] Train Acc: 0.589202 Loss: 1.394590\n",
            "[003/300] Train Acc: 0.631506 Loss: 1.239109\n",
            "[004/300] Train Acc: 0.657273 Loss: 1.146511\n",
            "[005/300] Train Acc: 0.674855 Loss: 1.082932\n",
            "[006/300] Train Acc: 0.687993 Loss: 1.034096\n",
            "[007/300] Train Acc: 0.698865 Loss: 0.992555\n",
            "[008/300] Train Acc: 0.709210 Loss: 0.959811\n",
            "[009/300] Train Acc: 0.716937 Loss: 0.930360\n",
            "[010/300] Train Acc: 0.724081 Loss: 0.905309\n",
            "[011/300] Train Acc: 0.730530 Loss: 0.881355\n",
            "[012/300] Train Acc: 0.735908 Loss: 0.860807\n",
            "[013/300] Train Acc: 0.740832 Loss: 0.842603\n",
            "[014/300] Train Acc: 0.746223 Loss: 0.824322\n",
            "[015/300] Train Acc: 0.750329 Loss: 0.809233\n",
            "[016/300] Train Acc: 0.754263 Loss: 0.795010\n",
            "[017/300] Train Acc: 0.757430 Loss: 0.781823\n",
            "[018/300] Train Acc: 0.761398 Loss: 0.769405\n",
            "[019/300] Train Acc: 0.763933 Loss: 0.758426\n",
            "[020/300] Train Acc: 0.767190 Loss: 0.749261\n",
            "[021/300] Train Acc: 0.770095 Loss: 0.737424\n",
            "[022/300] Train Acc: 0.772890 Loss: 0.728403\n",
            "[023/300] Train Acc: 0.775192 Loss: 0.719306\n",
            "[024/300] Train Acc: 0.777894 Loss: 0.711425\n",
            "[025/300] Train Acc: 0.779367 Loss: 0.702989\n",
            "[026/300] Train Acc: 0.782059 Loss: 0.694304\n",
            "[027/300] Train Acc: 0.784175 Loss: 0.687189\n",
            "[028/300] Train Acc: 0.785740 Loss: 0.680648\n",
            "[029/300] Train Acc: 0.787234 Loss: 0.674693\n",
            "[030/300] Train Acc: 0.789593 Loss: 0.668302\n",
            "[031/300] Train Acc: 0.790817 Loss: 0.663754\n",
            "[032/300] Train Acc: 0.792429 Loss: 0.657387\n",
            "[033/300] Train Acc: 0.794259 Loss: 0.650831\n",
            "[034/300] Train Acc: 0.795486 Loss: 0.646295\n",
            "[035/300] Train Acc: 0.796291 Loss: 0.643123\n",
            "[036/300] Train Acc: 0.798135 Loss: 0.636193\n",
            "[037/300] Train Acc: 0.799582 Loss: 0.630249\n",
            "[038/300] Train Acc: 0.800672 Loss: 0.627629\n",
            "[039/300] Train Acc: 0.801781 Loss: 0.624153\n",
            "[040/300] Train Acc: 0.803035 Loss: 0.619388\n",
            "[041/300] Train Acc: 0.804159 Loss: 0.615251\n",
            "[042/300] Train Acc: 0.804771 Loss: 0.611868\n",
            "[043/300] Train Acc: 0.806485 Loss: 0.606546\n",
            "[044/300] Train Acc: 0.807277 Loss: 0.604489\n",
            "[045/300] Train Acc: 0.808048 Loss: 0.600645\n",
            "[046/300] Train Acc: 0.809237 Loss: 0.597392\n",
            "[047/300] Train Acc: 0.809806 Loss: 0.595159\n",
            "[048/300] Train Acc: 0.811288 Loss: 0.590101\n",
            "[049/300] Train Acc: 0.812107 Loss: 0.586844\n",
            "[050/300] Train Acc: 0.812712 Loss: 0.584034\n",
            "[051/300] Train Acc: 0.813290 Loss: 0.580944\n",
            "[052/300] Train Acc: 0.814008 Loss: 0.578515\n",
            "[053/300] Train Acc: 0.815114 Loss: 0.576186\n",
            "[054/300] Train Acc: 0.816535 Loss: 0.571424\n",
            "[055/300] Train Acc: 0.816359 Loss: 0.570608\n",
            "[056/300] Train Acc: 0.816994 Loss: 0.567472\n",
            "[057/300] Train Acc: 0.818670 Loss: 0.564622\n",
            "[058/300] Train Acc: 0.818542 Loss: 0.562465\n",
            "[059/300] Train Acc: 0.819334 Loss: 0.559945\n",
            "[060/300] Train Acc: 0.820105 Loss: 0.557954\n",
            "[061/300] Train Acc: 0.821309 Loss: 0.555119\n",
            "[062/300] Train Acc: 0.821578 Loss: 0.554013\n",
            "[063/300] Train Acc: 0.821989 Loss: 0.551298\n",
            "[064/300] Train Acc: 0.822509 Loss: 0.548573\n",
            "[065/300] Train Acc: 0.823345 Loss: 0.547226\n",
            "[066/300] Train Acc: 0.823530 Loss: 0.546004\n",
            "[067/300] Train Acc: 0.824185 Loss: 0.543194\n",
            "[068/300] Train Acc: 0.824836 Loss: 0.541043\n",
            "[069/300] Train Acc: 0.825424 Loss: 0.540095\n",
            "[070/300] Train Acc: 0.826135 Loss: 0.536548\n",
            "[071/300] Train Acc: 0.826548 Loss: 0.535704\n",
            "[072/300] Train Acc: 0.826673 Loss: 0.534351\n",
            "[073/300] Train Acc: 0.827808 Loss: 0.531720\n",
            "[074/300] Train Acc: 0.827733 Loss: 0.530404\n",
            "[075/300] Train Acc: 0.828779 Loss: 0.528083\n",
            "[076/300] Train Acc: 0.828708 Loss: 0.527414\n",
            "[077/300] Train Acc: 0.829348 Loss: 0.525245\n",
            "[078/300] Train Acc: 0.829695 Loss: 0.523220\n",
            "[079/300] Train Acc: 0.830197 Loss: 0.522468\n",
            "[080/300] Train Acc: 0.830669 Loss: 0.520193\n",
            "[081/300] Train Acc: 0.830895 Loss: 0.518246\n",
            "[082/300] Train Acc: 0.831667 Loss: 0.516980\n",
            "[083/300] Train Acc: 0.831966 Loss: 0.515396\n",
            "[084/300] Train Acc: 0.832468 Loss: 0.514236\n",
            "[085/300] Train Acc: 0.833344 Loss: 0.510992\n",
            "[086/300] Train Acc: 0.833391 Loss: 0.510011\n",
            "[087/300] Train Acc: 0.833565 Loss: 0.509612\n",
            "[088/300] Train Acc: 0.834175 Loss: 0.508301\n",
            "[089/300] Train Acc: 0.834675 Loss: 0.506646\n",
            "[090/300] Train Acc: 0.834642 Loss: 0.504749\n",
            "[091/300] Train Acc: 0.835213 Loss: 0.504042\n",
            "[092/300] Train Acc: 0.835445 Loss: 0.502237\n",
            "[093/300] Train Acc: 0.836094 Loss: 0.501405\n",
            "[094/300] Train Acc: 0.836606 Loss: 0.500173\n",
            "[095/300] Train Acc: 0.836399 Loss: 0.500605\n",
            "[096/300] Train Acc: 0.837114 Loss: 0.498127\n",
            "[097/300] Train Acc: 0.836873 Loss: 0.497115\n",
            "[098/300] Train Acc: 0.836872 Loss: 0.497645\n",
            "[099/300] Train Acc: 0.837524 Loss: 0.494959\n",
            "[100/300] Train Acc: 0.837946 Loss: 0.494453\n",
            "[101/300] Train Acc: 0.838496 Loss: 0.493135\n",
            "[102/300] Train Acc: 0.838238 Loss: 0.492654\n",
            "[103/300] Train Acc: 0.838872 Loss: 0.490667\n",
            "[104/300] Train Acc: 0.838668 Loss: 0.490398\n",
            "[105/300] Train Acc: 0.839362 Loss: 0.489023\n",
            "[106/300] Train Acc: 0.839568 Loss: 0.488473\n",
            "[107/300] Train Acc: 0.840011 Loss: 0.486918\n",
            "[108/300] Train Acc: 0.840010 Loss: 0.485221\n",
            "[109/300] Train Acc: 0.840238 Loss: 0.486075\n",
            "[110/300] Train Acc: 0.840468 Loss: 0.484916\n",
            "[111/300] Train Acc: 0.841151 Loss: 0.482888\n",
            "[112/300] Train Acc: 0.841615 Loss: 0.482462\n",
            "[113/300] Train Acc: 0.841407 Loss: 0.481895\n",
            "[114/300] Train Acc: 0.841513 Loss: 0.480724\n",
            "[115/300] Train Acc: 0.841755 Loss: 0.480736\n",
            "[116/300] Train Acc: 0.842289 Loss: 0.479638\n",
            "[117/300] Train Acc: 0.842048 Loss: 0.479066\n",
            "[118/300] Train Acc: 0.843078 Loss: 0.477012\n",
            "[119/300] Train Acc: 0.842586 Loss: 0.477029\n",
            "[120/300] Train Acc: 0.843239 Loss: 0.475843\n",
            "[121/300] Train Acc: 0.843557 Loss: 0.474549\n",
            "[122/300] Train Acc: 0.843890 Loss: 0.475156\n",
            "[123/300] Train Acc: 0.843679 Loss: 0.473495\n",
            "[124/300] Train Acc: 0.844208 Loss: 0.472575\n",
            "[125/300] Train Acc: 0.844192 Loss: 0.472803\n",
            "[126/300] Train Acc: 0.844257 Loss: 0.471114\n",
            "[127/300] Train Acc: 0.844947 Loss: 0.470702\n",
            "[128/300] Train Acc: 0.844308 Loss: 0.470085\n",
            "[129/300] Train Acc: 0.844907 Loss: 0.468509\n",
            "[130/300] Train Acc: 0.845320 Loss: 0.468760\n",
            "[131/300] Train Acc: 0.845439 Loss: 0.467387\n",
            "[132/300] Train Acc: 0.845336 Loss: 0.467693\n",
            "[133/300] Train Acc: 0.845917 Loss: 0.466990\n",
            "[134/300] Train Acc: 0.846123 Loss: 0.464858\n",
            "[135/300] Train Acc: 0.846331 Loss: 0.465407\n",
            "[136/300] Train Acc: 0.846311 Loss: 0.464221\n",
            "[137/300] Train Acc: 0.846339 Loss: 0.464732\n",
            "[138/300] Train Acc: 0.847161 Loss: 0.462705\n",
            "[139/300] Train Acc: 0.847257 Loss: 0.462442\n",
            "[140/300] Train Acc: 0.846857 Loss: 0.462260\n",
            "[141/300] Train Acc: 0.846881 Loss: 0.461795\n",
            "[142/300] Train Acc: 0.847277 Loss: 0.461156\n",
            "[143/300] Train Acc: 0.847385 Loss: 0.460864\n",
            "[144/300] Train Acc: 0.847788 Loss: 0.458886\n",
            "[145/300] Train Acc: 0.848119 Loss: 0.458882\n",
            "[146/300] Train Acc: 0.848074 Loss: 0.458666\n",
            "[147/300] Train Acc: 0.848188 Loss: 0.458172\n",
            "[148/300] Train Acc: 0.848410 Loss: 0.457057\n",
            "[149/300] Train Acc: 0.848621 Loss: 0.455996\n",
            "[150/300] Train Acc: 0.849139 Loss: 0.455988\n",
            "[151/300] Train Acc: 0.849173 Loss: 0.455368\n",
            "[152/300] Train Acc: 0.848966 Loss: 0.455137\n",
            "[153/300] Train Acc: 0.849224 Loss: 0.454183\n",
            "[154/300] Train Acc: 0.849491 Loss: 0.454189\n",
            "[155/300] Train Acc: 0.849619 Loss: 0.453058\n",
            "[156/300] Train Acc: 0.849915 Loss: 0.452164\n",
            "[157/300] Train Acc: 0.849679 Loss: 0.452472\n",
            "[158/300] Train Acc: 0.850034 Loss: 0.451541\n",
            "[159/300] Train Acc: 0.849954 Loss: 0.451068\n",
            "[160/300] Train Acc: 0.850349 Loss: 0.450127\n",
            "[161/300] Train Acc: 0.850360 Loss: 0.450423\n",
            "[162/300] Train Acc: 0.850802 Loss: 0.449826\n",
            "[163/300] Train Acc: 0.850765 Loss: 0.450078\n",
            "[164/300] Train Acc: 0.850601 Loss: 0.449223\n",
            "[165/300] Train Acc: 0.850843 Loss: 0.448505\n",
            "[166/300] Train Acc: 0.850836 Loss: 0.448357\n",
            "[167/300] Train Acc: 0.851284 Loss: 0.447709\n",
            "[168/300] Train Acc: 0.851598 Loss: 0.446629\n",
            "[169/300] Train Acc: 0.851580 Loss: 0.446006\n",
            "[170/300] Train Acc: 0.851317 Loss: 0.446070\n",
            "[171/300] Train Acc: 0.851474 Loss: 0.446135\n",
            "[172/300] Train Acc: 0.851753 Loss: 0.445717\n",
            "[173/300] Train Acc: 0.852331 Loss: 0.444558\n",
            "[174/300] Train Acc: 0.851714 Loss: 0.444893\n",
            "[175/300] Train Acc: 0.852537 Loss: 0.444052\n",
            "[176/300] Train Acc: 0.852337 Loss: 0.444341\n",
            "[177/300] Train Acc: 0.852638 Loss: 0.442706\n",
            "[178/300] Train Acc: 0.852543 Loss: 0.442720\n",
            "[179/300] Train Acc: 0.853075 Loss: 0.442392\n",
            "[180/300] Train Acc: 0.852759 Loss: 0.441436\n",
            "[181/300] Train Acc: 0.852750 Loss: 0.441425\n",
            "[182/300] Train Acc: 0.853023 Loss: 0.440554\n",
            "[183/300] Train Acc: 0.853166 Loss: 0.440988\n",
            "[184/300] Train Acc: 0.853305 Loss: 0.440206\n",
            "[185/300] Train Acc: 0.853723 Loss: 0.439253\n",
            "[186/300] Train Acc: 0.853663 Loss: 0.439291\n",
            "[187/300] Train Acc: 0.853655 Loss: 0.439001\n",
            "[188/300] Train Acc: 0.854009 Loss: 0.438671\n",
            "[189/300] Train Acc: 0.853620 Loss: 0.438249\n",
            "[190/300] Train Acc: 0.854059 Loss: 0.437254\n",
            "[191/300] Train Acc: 0.854204 Loss: 0.436465\n",
            "[192/300] Train Acc: 0.854221 Loss: 0.436331\n",
            "[193/300] Train Acc: 0.855038 Loss: 0.434884\n",
            "[194/300] Train Acc: 0.854437 Loss: 0.435429\n",
            "[195/300] Train Acc: 0.854234 Loss: 0.436692\n",
            "[196/300] Train Acc: 0.855004 Loss: 0.434916\n",
            "[197/300] Train Acc: 0.855150 Loss: 0.433765\n",
            "[198/300] Train Acc: 0.854994 Loss: 0.434537\n",
            "[199/300] Train Acc: 0.855011 Loss: 0.434304\n",
            "[200/300] Train Acc: 0.855398 Loss: 0.432722\n",
            "[201/300] Train Acc: 0.855312 Loss: 0.432627\n",
            "[202/300] Train Acc: 0.855376 Loss: 0.432083\n",
            "[203/300] Train Acc: 0.855679 Loss: 0.432407\n",
            "[204/300] Train Acc: 0.855868 Loss: 0.432275\n",
            "[205/300] Train Acc: 0.855790 Loss: 0.431050\n",
            "[206/300] Train Acc: 0.856141 Loss: 0.430899\n",
            "[207/300] Train Acc: 0.856062 Loss: 0.431660\n",
            "[208/300] Train Acc: 0.856216 Loss: 0.429848\n",
            "[209/300] Train Acc: 0.855861 Loss: 0.430019\n",
            "[210/300] Train Acc: 0.857059 Loss: 0.428383\n",
            "[211/300] Train Acc: 0.857021 Loss: 0.428871\n",
            "[212/300] Train Acc: 0.856992 Loss: 0.428536\n",
            "[213/300] Train Acc: 0.857086 Loss: 0.428117\n",
            "[214/300] Train Acc: 0.856668 Loss: 0.427794\n",
            "[215/300] Train Acc: 0.857027 Loss: 0.427288\n",
            "[216/300] Train Acc: 0.857369 Loss: 0.426970\n",
            "[217/300] Train Acc: 0.857036 Loss: 0.426381\n",
            "[218/300] Train Acc: 0.857739 Loss: 0.426064\n",
            "[219/300] Train Acc: 0.857352 Loss: 0.426748\n",
            "[220/300] Train Acc: 0.857827 Loss: 0.426047\n",
            "[221/300] Train Acc: 0.857375 Loss: 0.424862\n",
            "[222/300] Train Acc: 0.857464 Loss: 0.425473\n",
            "[223/300] Train Acc: 0.858257 Loss: 0.423688\n",
            "[224/300] Train Acc: 0.857584 Loss: 0.425150\n",
            "[225/300] Train Acc: 0.857759 Loss: 0.424522\n",
            "[226/300] Train Acc: 0.857851 Loss: 0.423552\n",
            "[227/300] Train Acc: 0.858476 Loss: 0.422329\n",
            "[228/300] Train Acc: 0.858735 Loss: 0.422929\n",
            "[229/300] Train Acc: 0.858099 Loss: 0.423135\n",
            "[230/300] Train Acc: 0.858439 Loss: 0.421875\n",
            "[231/300] Train Acc: 0.858791 Loss: 0.422435\n",
            "[232/300] Train Acc: 0.858508 Loss: 0.421693\n",
            "[233/300] Train Acc: 0.858573 Loss: 0.422113\n",
            "[234/300] Train Acc: 0.858930 Loss: 0.421445\n",
            "[235/300] Train Acc: 0.859180 Loss: 0.421306\n",
            "[236/300] Train Acc: 0.859278 Loss: 0.421296\n",
            "[237/300] Train Acc: 0.858484 Loss: 0.421526\n",
            "[238/300] Train Acc: 0.858749 Loss: 0.420714\n",
            "[239/300] Train Acc: 0.859447 Loss: 0.419932\n",
            "[240/300] Train Acc: 0.859120 Loss: 0.419668\n",
            "[241/300] Train Acc: 0.858967 Loss: 0.419864\n",
            "[242/300] Train Acc: 0.859390 Loss: 0.419510\n",
            "[243/300] Train Acc: 0.859980 Loss: 0.418041\n",
            "[244/300] Train Acc: 0.859504 Loss: 0.419121\n",
            "[245/300] Train Acc: 0.859554 Loss: 0.417944\n",
            "[246/300] Train Acc: 0.859636 Loss: 0.417838\n",
            "[247/300] Train Acc: 0.859644 Loss: 0.417504\n",
            "[248/300] Train Acc: 0.860135 Loss: 0.416490\n",
            "[249/300] Train Acc: 0.859774 Loss: 0.417052\n",
            "[250/300] Train Acc: 0.860034 Loss: 0.417219\n",
            "[251/300] Train Acc: 0.860239 Loss: 0.416990\n",
            "[252/300] Train Acc: 0.860241 Loss: 0.416085\n",
            "[253/300] Train Acc: 0.860100 Loss: 0.416196\n",
            "[254/300] Train Acc: 0.860564 Loss: 0.415547\n",
            "[255/300] Train Acc: 0.860344 Loss: 0.415835\n",
            "[256/300] Train Acc: 0.860576 Loss: 0.415737\n",
            "[257/300] Train Acc: 0.860904 Loss: 0.414796\n",
            "[258/300] Train Acc: 0.860761 Loss: 0.414376\n",
            "[259/300] Train Acc: 0.860891 Loss: 0.413817\n",
            "[260/300] Train Acc: 0.860751 Loss: 0.414807\n",
            "[261/300] Train Acc: 0.860754 Loss: 0.414072\n",
            "[262/300] Train Acc: 0.860940 Loss: 0.413433\n",
            "[263/300] Train Acc: 0.861375 Loss: 0.412817\n",
            "[264/300] Train Acc: 0.860731 Loss: 0.414379\n",
            "[265/300] Train Acc: 0.861444 Loss: 0.412416\n",
            "[266/300] Train Acc: 0.861188 Loss: 0.413124\n",
            "[267/300] Train Acc: 0.861935 Loss: 0.411344\n",
            "[268/300] Train Acc: 0.861241 Loss: 0.413002\n",
            "[269/300] Train Acc: 0.861234 Loss: 0.412580\n",
            "[270/300] Train Acc: 0.861366 Loss: 0.411757\n",
            "[271/300] Train Acc: 0.861882 Loss: 0.410533\n",
            "[272/300] Train Acc: 0.862010 Loss: 0.411094\n",
            "[273/300] Train Acc: 0.861392 Loss: 0.411597\n",
            "[274/300] Train Acc: 0.861918 Loss: 0.410659\n",
            "[275/300] Train Acc: 0.861792 Loss: 0.411149\n",
            "[276/300] Train Acc: 0.861959 Loss: 0.410321\n",
            "[277/300] Train Acc: 0.862248 Loss: 0.410293\n",
            "[278/300] Train Acc: 0.862525 Loss: 0.409992\n",
            "[279/300] Train Acc: 0.862114 Loss: 0.409605\n",
            "[280/300] Train Acc: 0.861985 Loss: 0.410437\n",
            "[281/300] Train Acc: 0.862297 Loss: 0.408885\n",
            "[282/300] Train Acc: 0.862539 Loss: 0.408896\n",
            "[283/300] Train Acc: 0.862435 Loss: 0.409262\n",
            "[284/300] Train Acc: 0.862950 Loss: 0.407495\n",
            "[285/300] Train Acc: 0.862746 Loss: 0.407712\n",
            "[286/300] Train Acc: 0.862877 Loss: 0.408517\n",
            "[287/300] Train Acc: 0.863028 Loss: 0.407500\n",
            "[288/300] Train Acc: 0.863017 Loss: 0.406961\n",
            "[289/300] Train Acc: 0.862477 Loss: 0.408308\n",
            "[290/300] Train Acc: 0.862718 Loss: 0.407024\n",
            "[291/300] Train Acc: 0.862981 Loss: 0.407147\n",
            "[292/300] Train Acc: 0.863044 Loss: 0.406329\n",
            "[293/300] Train Acc: 0.863605 Loss: 0.405964\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDbv5ysTepyl"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/CNNmodel_25.ckpt\" \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PKjtAScPWtr"
      },
      "outputs": [],
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [],
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCxMZjK0sR2R"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(1, len(predict)-1):\n",
        "    step = 1\n",
        "    previous_ = predict[i-step]\n",
        "    next_ = predict[i+step]\n",
        "    current_ = predict[i]\n",
        "    if (previous_ != current_) and (next_ != current_) and (previous_ == next_):\n",
        "        print('idx',i,'correct', current_, 'to', previous_)\n",
        "        predict[i] = previous_\n",
        "        count +=1\n",
        "\n",
        "print('total number of correction %d, correction percent %.2f'% (count, count/len(predict)))\n",
        "\n",
        "for i in range(1, len(predict)-1):\n",
        "    step = 2\n",
        "    if i == 1 or i == len(predict)-2: step = 1\n",
        "    previous_ = predict[i-step]\n",
        "    next_ = predict[i+step]\n",
        "    current_ = predict[i]\n",
        "    if (previous_ != current_) and (next_ != current_) and (previous_ == next_):\n",
        "        print('idx',i,'correct', current_, 'to', previous_)\n",
        "        predict[i] = previous_\n",
        "        count +=1\n",
        "\n",
        "print('total number of correction %d, correction percent %.2f'% (count, count/len(predict)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('predictionCNN_25.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6-drq6V8cBJ"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/predictionCNN_25.csv\" \"/content/drive/MyDrive/\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}