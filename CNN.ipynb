{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fockspaces/ML2021spring/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n",
        "* Video (English): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqKyrT9mDtM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk_CWeC8mDR0",
        "outputId": "9e968d57-7e88-4937-8ee3-db0182012d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "d9684f8a-6974-423d-b5bd-76903cfbfa4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  drive  sample_data  timit_11\n"
          ]
        }
      ],
      "source": [
        "# !gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!cp \"/content/drive/MyDrive/timit_11_v2.zip 的副本\" \"data.zip\"\n",
        "!unzip data.zip\n",
        "!ls "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "79408fda-8ba6-4721-dab1-1946ed81068c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WB9xXUlIqAT"
      },
      "outputs": [],
      "source": [
        "def sample(data, stride):\n",
        "  base = data.copy()\n",
        "  for step in range(1, stride+1, 1):\n",
        "    Rshift = np.roll(base,step,axis=0)\n",
        "    data = np.concatenate((Rshift,data), axis=1)\n",
        "  for step in range(-1, -1-1*stride, -1):\n",
        "    Lshift = np.roll(base,step,axis=0)\n",
        "    data = np.concatenate((data,Lshift), axis=1)\n",
        "\n",
        "  data = np.reshape(data, (-1,2*stride+1,39))\n",
        "  return data\n",
        "\n",
        "def normalize(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  std = np.std(data, axis=0)\n",
        "  return (data - mean)/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_0i5dpHQPBu"
      },
      "outputs": [],
      "source": [
        "stride = 10\n",
        "# (,429)split to(11,39)\n",
        "train = np.reshape(train, (-1,11,39))\n",
        "test = np.reshape(test, (-1,11,39))\n",
        "\n",
        "# pick only the 5th MFCC which is corresponding to the label\n",
        "train = train[:,5,:]\n",
        "test = test[:,5,:]\n",
        "\n",
        "# include nearby MFCC (To extend the frame length)\n",
        "train = sample(train, stride)\n",
        "test = sample(test, stride)\n",
        "\n",
        "# transform to 4d input array (with kernel)\n",
        "train = np.reshape(train, (-1,1,(2*stride+1),39))\n",
        "test = np.reshape(test, (-1,1,(2*stride+1),39))\n",
        "\n",
        "# normalize data\n",
        "# train = normalize(train)\n",
        "# test = normalize(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsKh0UbeO5VY",
        "outputId": "653b64c2-cc11-40ae-afd6-12050d319b8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1229932, 1, 21, 39)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "siKG6cmMepVv",
        "outputId": "7686d2fb-931d-4359-dc09-bdc5cc017f96"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "mfcc = np.reshape(train[0], ((2*stride+1),39))\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.imshow(mfcc);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYqi_lAuvC59"
      },
      "outputs": [],
      "source": [
        "VAL_RATIO = 0\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "# train_x, train_y, val_x, val_y = train[:], train_label[:], train[percent:], train_label[percent:]\n",
        "val_x, val_y = train[percent:], train_label[percent:]\n",
        "# print('Size of training set: {}'.format(train_x.shape))\n",
        "# print('Size of validation set: {}'.format(val_x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E04qGvVN212c"
      },
      "outputs": [],
      "source": [
        "# train_x = train_x.reshape(train_x.shape[0], 1, train_x.shape[1], -1)\n",
        "# test_x = test.reshape(test.shape[0], 1, test.shape[1], -1)\n",
        "# train_x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUCbQvqJurYc",
        "outputId": "f2efc879-5cd3-41eb-e47c-69d4a49c9416"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train, train_label)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "9cc03dd8-5859-40fc-ff5a-8f7710486754"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, val_x, val_y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.layer3 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "\n",
        "        self.act_fn = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfJfs6CcXZZj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # The arguments for commonly used modules:\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "        # input image size: [1, num, 29]\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            #1st\n",
        "            nn.Conv2d(1, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            #2nd\n",
        "            nn.Conv2d(32, 32, 5, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            #3rd\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #4th\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            #5rd\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #6th\n",
        "            nn.Conv2d(128, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(16 * 8*8, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 39)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input (x): [batch_size, 3, 128, 128]\n",
        "        # output: [batch_size, 11]\n",
        "\n",
        "        # Extract features by convolutional layers.\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
        "        x = self.fc_layers(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "outputs": [],
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "outputs": [],
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp3ZXg1yO9Y",
        "outputId": "49db8d88-9d38-4685-ac35-370b7c9c5399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 300               # number of training epoch\n",
        "learning_rate = 1e-4       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './CNNmodel.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "c02de593-5cf2-4a7d-f116-f8b8f69d7cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/300] Train Acc: 0.462763 Loss: 1.910739\n",
            "[002/300] Train Acc: 0.605814 Loss: 1.325099\n",
            "[003/300] Train Acc: 0.645224 Loss: 1.179467\n",
            "[004/300] Train Acc: 0.668103 Loss: 1.098525\n",
            "[005/300] Train Acc: 0.683301 Loss: 1.042289\n",
            "[006/300] Train Acc: 0.694838 Loss: 0.999062\n",
            "[007/300] Train Acc: 0.704224 Loss: 0.963265\n",
            "[008/300] Train Acc: 0.712342 Loss: 0.935720\n",
            "[009/300] Train Acc: 0.719807 Loss: 0.909164\n",
            "[010/300] Train Acc: 0.725100 Loss: 0.888712\n",
            "[011/300] Train Acc: 0.730375 Loss: 0.868956\n",
            "[012/300] Train Acc: 0.735272 Loss: 0.851639\n",
            "[013/300] Train Acc: 0.739306 Loss: 0.835731\n",
            "[014/300] Train Acc: 0.743583 Loss: 0.821465\n",
            "[015/300] Train Acc: 0.747020 Loss: 0.808484\n",
            "[016/300] Train Acc: 0.750260 Loss: 0.795936\n",
            "[017/300] Train Acc: 0.753760 Loss: 0.783113\n",
            "[018/300] Train Acc: 0.756684 Loss: 0.773097\n",
            "[019/300] Train Acc: 0.759351 Loss: 0.764050\n",
            "[020/300] Train Acc: 0.761801 Loss: 0.754114\n",
            "[021/300] Train Acc: 0.764289 Loss: 0.746209\n",
            "[022/300] Train Acc: 0.766802 Loss: 0.736643\n",
            "[023/300] Train Acc: 0.768464 Loss: 0.729664\n",
            "[024/300] Train Acc: 0.770938 Loss: 0.722095\n",
            "[025/300] Train Acc: 0.772900 Loss: 0.715310\n",
            "[026/300] Train Acc: 0.774691 Loss: 0.708387\n",
            "[027/300] Train Acc: 0.776185 Loss: 0.702608\n",
            "[028/300] Train Acc: 0.778246 Loss: 0.696616\n",
            "[029/300] Train Acc: 0.779388 Loss: 0.691264\n",
            "[030/300] Train Acc: 0.781455 Loss: 0.685350\n",
            "[031/300] Train Acc: 0.782453 Loss: 0.680219\n",
            "[032/300] Train Acc: 0.783834 Loss: 0.675384\n",
            "[033/300] Train Acc: 0.785350 Loss: 0.670428\n",
            "[034/300] Train Acc: 0.786774 Loss: 0.665724\n",
            "[035/300] Train Acc: 0.787904 Loss: 0.661589\n",
            "[036/300] Train Acc: 0.789076 Loss: 0.656637\n",
            "[037/300] Train Acc: 0.790089 Loss: 0.652994\n",
            "[038/300] Train Acc: 0.791701 Loss: 0.648135\n",
            "[039/300] Train Acc: 0.791971 Loss: 0.645570\n",
            "[040/300] Train Acc: 0.793685 Loss: 0.641552\n",
            "[041/300] Train Acc: 0.794309 Loss: 0.638745\n",
            "[042/300] Train Acc: 0.795341 Loss: 0.635543\n",
            "[043/300] Train Acc: 0.796252 Loss: 0.631915\n",
            "[044/300] Train Acc: 0.797657 Loss: 0.627549\n",
            "[045/300] Train Acc: 0.798208 Loss: 0.625193\n",
            "[046/300] Train Acc: 0.799160 Loss: 0.622003\n",
            "[047/300] Train Acc: 0.799711 Loss: 0.619735\n",
            "[048/300] Train Acc: 0.800824 Loss: 0.616953\n",
            "[049/300] Train Acc: 0.801369 Loss: 0.614514\n",
            "[050/300] Train Acc: 0.801991 Loss: 0.611962\n",
            "[051/300] Train Acc: 0.803120 Loss: 0.609094\n",
            "[052/300] Train Acc: 0.803129 Loss: 0.607554\n",
            "[053/300] Train Acc: 0.804319 Loss: 0.603538\n",
            "[054/300] Train Acc: 0.805276 Loss: 0.600742\n",
            "[055/300] Train Acc: 0.805789 Loss: 0.598573\n",
            "[056/300] Train Acc: 0.806358 Loss: 0.596621\n",
            "[057/300] Train Acc: 0.806995 Loss: 0.595258\n",
            "[058/300] Train Acc: 0.807584 Loss: 0.592484\n",
            "[059/300] Train Acc: 0.808319 Loss: 0.590712\n",
            "[060/300] Train Acc: 0.808302 Loss: 0.588640\n",
            "[061/300] Train Acc: 0.808909 Loss: 0.586409\n",
            "[062/300] Train Acc: 0.809937 Loss: 0.584363\n",
            "[063/300] Train Acc: 0.810682 Loss: 0.582398\n",
            "[064/300] Train Acc: 0.810975 Loss: 0.581109\n",
            "[065/300] Train Acc: 0.811490 Loss: 0.578873\n",
            "[066/300] Train Acc: 0.812155 Loss: 0.576617\n",
            "[067/300] Train Acc: 0.812407 Loss: 0.574207\n",
            "[068/300] Train Acc: 0.813284 Loss: 0.572900\n",
            "[069/300] Train Acc: 0.813672 Loss: 0.571704\n",
            "[070/300] Train Acc: 0.813945 Loss: 0.569698\n",
            "[071/300] Train Acc: 0.814177 Loss: 0.568475\n",
            "[072/300] Train Acc: 0.814756 Loss: 0.566716\n",
            "[073/300] Train Acc: 0.815377 Loss: 0.565152\n",
            "[074/300] Train Acc: 0.815384 Loss: 0.564129\n",
            "[075/300] Train Acc: 0.816005 Loss: 0.562697\n",
            "[076/300] Train Acc: 0.816906 Loss: 0.560384\n",
            "[077/300] Train Acc: 0.816880 Loss: 0.559264\n",
            "[078/300] Train Acc: 0.817343 Loss: 0.558361\n",
            "[079/300] Train Acc: 0.817838 Loss: 0.556573\n",
            "[080/300] Train Acc: 0.817900 Loss: 0.554966\n",
            "[081/300] Train Acc: 0.818558 Loss: 0.553825\n",
            "[082/300] Train Acc: 0.818699 Loss: 0.553007\n",
            "[083/300] Train Acc: 0.819350 Loss: 0.551436\n",
            "[084/300] Train Acc: 0.819548 Loss: 0.550175\n",
            "[085/300] Train Acc: 0.819858 Loss: 0.548631\n",
            "[086/300] Train Acc: 0.820730 Loss: 0.547050\n",
            "[087/300] Train Acc: 0.821038 Loss: 0.545847\n",
            "[088/300] Train Acc: 0.820993 Loss: 0.545016\n",
            "[089/300] Train Acc: 0.821317 Loss: 0.544228\n",
            "[090/300] Train Acc: 0.821920 Loss: 0.542725\n",
            "[091/300] Train Acc: 0.821848 Loss: 0.541915\n",
            "[092/300] Train Acc: 0.822764 Loss: 0.539851\n",
            "[093/300] Train Acc: 0.822498 Loss: 0.538993\n",
            "[094/300] Train Acc: 0.823387 Loss: 0.537884\n",
            "[095/300] Train Acc: 0.823236 Loss: 0.537319\n",
            "[096/300] Train Acc: 0.823770 Loss: 0.536038\n",
            "[097/300] Train Acc: 0.824101 Loss: 0.535504\n",
            "[098/300] Train Acc: 0.824394 Loss: 0.534545\n",
            "[099/300] Train Acc: 0.824611 Loss: 0.532969\n",
            "[100/300] Train Acc: 0.825109 Loss: 0.531172\n",
            "[101/300] Train Acc: 0.825363 Loss: 0.530798\n",
            "[102/300] Train Acc: 0.825427 Loss: 0.530706\n",
            "[103/300] Train Acc: 0.825620 Loss: 0.528825\n",
            "[104/300] Train Acc: 0.826029 Loss: 0.529501\n",
            "[105/300] Train Acc: 0.826213 Loss: 0.527048\n",
            "[106/300] Train Acc: 0.826953 Loss: 0.525592\n",
            "[107/300] Train Acc: 0.826929 Loss: 0.525249\n",
            "[108/300] Train Acc: 0.826922 Loss: 0.523499\n",
            "[109/300] Train Acc: 0.827284 Loss: 0.524208\n",
            "[110/300] Train Acc: 0.828189 Loss: 0.520991\n",
            "[111/300] Train Acc: 0.827471 Loss: 0.521359\n",
            "[112/300] Train Acc: 0.827924 Loss: 0.520741\n",
            "[113/300] Train Acc: 0.828599 Loss: 0.519816\n",
            "[114/300] Train Acc: 0.828895 Loss: 0.518853\n",
            "[115/300] Train Acc: 0.829077 Loss: 0.518085\n",
            "[116/300] Train Acc: 0.829499 Loss: 0.517149\n",
            "[117/300] Train Acc: 0.829113 Loss: 0.516856\n",
            "[118/300] Train Acc: 0.829591 Loss: 0.516566\n",
            "[119/300] Train Acc: 0.830335 Loss: 0.514402\n",
            "[120/300] Train Acc: 0.830019 Loss: 0.514124\n",
            "[121/300] Train Acc: 0.830491 Loss: 0.513167\n",
            "[122/300] Train Acc: 0.830547 Loss: 0.513084\n",
            "[123/300] Train Acc: 0.830843 Loss: 0.511414\n",
            "[124/300] Train Acc: 0.831261 Loss: 0.510454\n",
            "[125/300] Train Acc: 0.831295 Loss: 0.509778\n",
            "[126/300] Train Acc: 0.832034 Loss: 0.508091\n",
            "[127/300] Train Acc: 0.831572 Loss: 0.509738\n",
            "[128/300] Train Acc: 0.831685 Loss: 0.508203\n",
            "[129/300] Train Acc: 0.831948 Loss: 0.507509\n",
            "[130/300] Train Acc: 0.832457 Loss: 0.506102\n",
            "[131/300] Train Acc: 0.832489 Loss: 0.505726\n",
            "[132/300] Train Acc: 0.832217 Loss: 0.505702\n",
            "[133/300] Train Acc: 0.832995 Loss: 0.503419\n",
            "[134/300] Train Acc: 0.832425 Loss: 0.504442\n",
            "[135/300] Train Acc: 0.833175 Loss: 0.504219\n",
            "[136/300] Train Acc: 0.833672 Loss: 0.502575\n",
            "[137/300] Train Acc: 0.833343 Loss: 0.501710\n",
            "[138/300] Train Acc: 0.834016 Loss: 0.500849\n",
            "[139/300] Train Acc: 0.834023 Loss: 0.500030\n",
            "[140/300] Train Acc: 0.834124 Loss: 0.499709\n",
            "[141/300] Train Acc: 0.834023 Loss: 0.499539\n",
            "[142/300] Train Acc: 0.834471 Loss: 0.499684\n",
            "[143/300] Train Acc: 0.834355 Loss: 0.498887\n",
            "[144/300] Train Acc: 0.835052 Loss: 0.497365\n",
            "[145/300] Train Acc: 0.835091 Loss: 0.496869\n",
            "[146/300] Train Acc: 0.834719 Loss: 0.497340\n",
            "[147/300] Train Acc: 0.835358 Loss: 0.495611\n",
            "[148/300] Train Acc: 0.835735 Loss: 0.494912\n",
            "[149/300] Train Acc: 0.835684 Loss: 0.496054\n",
            "[150/300] Train Acc: 0.835758 Loss: 0.494864\n",
            "[151/300] Train Acc: 0.835515 Loss: 0.494532\n",
            "[152/300] Train Acc: 0.836420 Loss: 0.492620\n",
            "[153/300] Train Acc: 0.836357 Loss: 0.493239\n",
            "[154/300] Train Acc: 0.836075 Loss: 0.493393\n",
            "[155/300] Train Acc: 0.836692 Loss: 0.492424\n",
            "[156/300] Train Acc: 0.836477 Loss: 0.491843\n",
            "[157/300] Train Acc: 0.837195 Loss: 0.490144\n",
            "[158/300] Train Acc: 0.837058 Loss: 0.489807\n",
            "[159/300] Train Acc: 0.837276 Loss: 0.489464\n",
            "[160/300] Train Acc: 0.837278 Loss: 0.489744\n",
            "[161/300] Train Acc: 0.838093 Loss: 0.488214\n",
            "[162/300] Train Acc: 0.837794 Loss: 0.487383\n",
            "[163/300] Train Acc: 0.837642 Loss: 0.487877\n",
            "[164/300] Train Acc: 0.837894 Loss: 0.486728\n",
            "[165/300] Train Acc: 0.838023 Loss: 0.486053\n",
            "[166/300] Train Acc: 0.838746 Loss: 0.484887\n",
            "[167/300] Train Acc: 0.838054 Loss: 0.486230\n",
            "[168/300] Train Acc: 0.838481 Loss: 0.485173\n",
            "[169/300] Train Acc: 0.838233 Loss: 0.485011\n",
            "[170/300] Train Acc: 0.838093 Loss: 0.485316\n",
            "[171/300] Train Acc: 0.838646 Loss: 0.483806\n",
            "[172/300] Train Acc: 0.838726 Loss: 0.482583\n",
            "[173/300] Train Acc: 0.838863 Loss: 0.483756\n",
            "[174/300] Train Acc: 0.839388 Loss: 0.482057\n",
            "[175/300] Train Acc: 0.839981 Loss: 0.480571\n",
            "[176/300] Train Acc: 0.839707 Loss: 0.481396\n",
            "[177/300] Train Acc: 0.839698 Loss: 0.481199\n",
            "[178/300] Train Acc: 0.839516 Loss: 0.480929\n",
            "[179/300] Train Acc: 0.840197 Loss: 0.480962\n",
            "[180/300] Train Acc: 0.840033 Loss: 0.479858\n",
            "[181/300] Train Acc: 0.840429 Loss: 0.478766\n",
            "[182/300] Train Acc: 0.840261 Loss: 0.478822\n",
            "[183/300] Train Acc: 0.840655 Loss: 0.478036\n",
            "[184/300] Train Acc: 0.840516 Loss: 0.478791\n",
            "[185/300] Train Acc: 0.840204 Loss: 0.478098\n",
            "[186/300] Train Acc: 0.841056 Loss: 0.476776\n",
            "[187/300] Train Acc: 0.841424 Loss: 0.476133\n",
            "[188/300] Train Acc: 0.841183 Loss: 0.476281\n",
            "[189/300] Train Acc: 0.841690 Loss: 0.474197\n",
            "[190/300] Train Acc: 0.841127 Loss: 0.475877\n",
            "[191/300] Train Acc: 0.841563 Loss: 0.475806\n",
            "[192/300] Train Acc: 0.841798 Loss: 0.474391\n",
            "[193/300] Train Acc: 0.841512 Loss: 0.475052\n",
            "[194/300] Train Acc: 0.841858 Loss: 0.473990\n",
            "[195/300] Train Acc: 0.841521 Loss: 0.473993\n",
            "[196/300] Train Acc: 0.841464 Loss: 0.473855\n",
            "[197/300] Train Acc: 0.842364 Loss: 0.472665\n",
            "[198/300] Train Acc: 0.842158 Loss: 0.472295\n",
            "[199/300] Train Acc: 0.842251 Loss: 0.472368\n",
            "[200/300] Train Acc: 0.842470 Loss: 0.471969\n",
            "[201/300] Train Acc: 0.842028 Loss: 0.472432\n",
            "[202/300] Train Acc: 0.842580 Loss: 0.470533\n",
            "[203/300] Train Acc: 0.842838 Loss: 0.470836\n",
            "[204/300] Train Acc: 0.842424 Loss: 0.471165\n",
            "[205/300] Train Acc: 0.843065 Loss: 0.469929\n",
            "[206/300] Train Acc: 0.842717 Loss: 0.469240\n",
            "[207/300] Train Acc: 0.843073 Loss: 0.469429\n",
            "[208/300] Train Acc: 0.843410 Loss: 0.468704\n",
            "[209/300] Train Acc: 0.843370 Loss: 0.468899\n",
            "[210/300] Train Acc: 0.843460 Loss: 0.468217\n",
            "[211/300] Train Acc: 0.843376 Loss: 0.467870\n",
            "[212/300] Train Acc: 0.843517 Loss: 0.467851\n",
            "[213/300] Train Acc: 0.843933 Loss: 0.466817\n",
            "[214/300] Train Acc: 0.844286 Loss: 0.465654\n",
            "[215/300] Train Acc: 0.844187 Loss: 0.465712\n",
            "[216/300] Train Acc: 0.843891 Loss: 0.465491\n",
            "[217/300] Train Acc: 0.844215 Loss: 0.465749\n",
            "[218/300] Train Acc: 0.844185 Loss: 0.465124\n",
            "[219/300] Train Acc: 0.844371 Loss: 0.464702\n",
            "[220/300] Train Acc: 0.844552 Loss: 0.464495\n",
            "[221/300] Train Acc: 0.844499 Loss: 0.463829\n",
            "[222/300] Train Acc: 0.844730 Loss: 0.463758\n",
            "[223/300] Train Acc: 0.845330 Loss: 0.463538\n",
            "[224/300] Train Acc: 0.844697 Loss: 0.463618\n",
            "[225/300] Train Acc: 0.844859 Loss: 0.463825\n",
            "[226/300] Train Acc: 0.844965 Loss: 0.463272\n",
            "[227/300] Train Acc: 0.845248 Loss: 0.463170\n",
            "[228/300] Train Acc: 0.845222 Loss: 0.462067\n",
            "[229/300] Train Acc: 0.845107 Loss: 0.462345\n",
            "[230/300] Train Acc: 0.845129 Loss: 0.462728\n",
            "[231/300] Train Acc: 0.845396 Loss: 0.461814\n",
            "[232/300] Train Acc: 0.845470 Loss: 0.461118\n",
            "[233/300] Train Acc: 0.845991 Loss: 0.460312\n",
            "[234/300] Train Acc: 0.846048 Loss: 0.460599\n",
            "[235/300] Train Acc: 0.845451 Loss: 0.460534\n",
            "[236/300] Train Acc: 0.845430 Loss: 0.461231\n",
            "[237/300] Train Acc: 0.845835 Loss: 0.459548\n",
            "[238/300] Train Acc: 0.845742 Loss: 0.459845\n",
            "[239/300] Train Acc: 0.846093 Loss: 0.458713\n",
            "[240/300] Train Acc: 0.846096 Loss: 0.458845\n",
            "[241/300] Train Acc: 0.845891 Loss: 0.458865\n",
            "[242/300] Train Acc: 0.846809 Loss: 0.457392\n",
            "[243/300] Train Acc: 0.846555 Loss: 0.457564\n",
            "[244/300] Train Acc: 0.847028 Loss: 0.457904\n",
            "[245/300] Train Acc: 0.846409 Loss: 0.458054\n",
            "[246/300] Train Acc: 0.846890 Loss: 0.456907\n",
            "[247/300] Train Acc: 0.846755 Loss: 0.457170\n",
            "[248/300] Train Acc: 0.847034 Loss: 0.456153\n",
            "[249/300] Train Acc: 0.846957 Loss: 0.457038\n",
            "[250/300] Train Acc: 0.847005 Loss: 0.455261\n",
            "[251/300] Train Acc: 0.847394 Loss: 0.455271\n",
            "[252/300] Train Acc: 0.846918 Loss: 0.455886\n",
            "[253/300] Train Acc: 0.847586 Loss: 0.454899\n",
            "[254/300] Train Acc: 0.847190 Loss: 0.455011\n",
            "[255/300] Train Acc: 0.847492 Loss: 0.454659\n",
            "[256/300] Train Acc: 0.847792 Loss: 0.453821\n",
            "[257/300] Train Acc: 0.847692 Loss: 0.453477\n",
            "[258/300] Train Acc: 0.847664 Loss: 0.453810\n",
            "[259/300] Train Acc: 0.847238 Loss: 0.454128\n",
            "[260/300] Train Acc: 0.848357 Loss: 0.453567\n",
            "[261/300] Train Acc: 0.848599 Loss: 0.452380\n",
            "[262/300] Train Acc: 0.847667 Loss: 0.453401\n",
            "[263/300] Train Acc: 0.847642 Loss: 0.453294\n",
            "[264/300] Train Acc: 0.848251 Loss: 0.452316\n",
            "[265/300] Train Acc: 0.848754 Loss: 0.450565\n",
            "[266/300] Train Acc: 0.848471 Loss: 0.451677\n",
            "[267/300] Train Acc: 0.848087 Loss: 0.452012\n",
            "[268/300] Train Acc: 0.847966 Loss: 0.451992\n",
            "[269/300] Train Acc: 0.848714 Loss: 0.450513\n",
            "[270/300] Train Acc: 0.848899 Loss: 0.450327\n",
            "[271/300] Train Acc: 0.848737 Loss: 0.449734\n",
            "[272/300] Train Acc: 0.848712 Loss: 0.450577\n",
            "[273/300] Train Acc: 0.848873 Loss: 0.450263\n",
            "[274/300] Train Acc: 0.848925 Loss: 0.450562\n",
            "[275/300] Train Acc: 0.849058 Loss: 0.449454\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDbv5ysTepyl"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/CNNmodel.ckpt\" \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PKjtAScPWtr"
      },
      "outputs": [],
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [],
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCxMZjK0sR2R"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(1, len(predict)-1):\n",
        "    previous_ = predict[i-1]\n",
        "    next_ = predict[i+1]\n",
        "    current_ = predict[i]\n",
        "    if (previous_ != current_) and (next_ != current_) and (previous_ == next_):\n",
        "        print('idx',i,'correct', current_, 'to', previous_)\n",
        "        predict[i] = previous_\n",
        "        count +=1\n",
        "\n",
        "print('total number of correction %d, correction percent %.2f'% (count, count/len(predict)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('predictionCNN.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6-drq6V8cBJ"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/predictionCNN.csv\" \"/content/drive/MyDrive/\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "simplemodel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}